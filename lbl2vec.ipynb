{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lbl2vec \n",
    "### Made by Taiki Papandreou and Thomas Meeusen\n",
    "\n",
    "This notebook illustarates how lbl2vec works on the provided Huggingface dataset. The original notebook was made by @sebischair. \n",
    "\n",
    "Link to the github repository: https://github.com/sebischair/Lbl2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set: \n",
    "https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv\n",
    "\n",
    "The AG's news topic classification dataset is constructed by choosing the 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600. \n",
    "\n",
    "The classes are: \n",
    "\n",
    "* World\n",
    "* Sports\n",
    "* Business\n",
    "* Science/Technology\n",
    "\n",
    "#### For more information on how to use Lbl2Vec, visit the [API Guide](https://lbl2vec.readthedocs.io/en/latest/api.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lbl2vec import Lbl2Vec\n",
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "ag_train = pd.read_csv('data/train.csv',sep=',',header=None, names=['class','title','description'], skiprows=1)\n",
    "\n",
    "# load test data\n",
    "ag_test = pd.read_csv('data/test.csv',sep=',',header=None, names=['class','title','description'], skiprows=1)\n",
    "\n",
    "# load labels with keywords\n",
    "labels = pd.read_csv('data/labels.csv',sep=';')\n",
    "\n",
    "# split keywords by separator and save them as array\n",
    "labels['keywords'] = labels['keywords'].apply(lambda x: x.split(' '))\n",
    "\n",
    "# convert description keywords to lowercase\n",
    "labels['keywords'] = labels['keywords'].apply(lambda description_keywords: [keyword.lower() for keyword in description_keywords])\n",
    "\n",
    "# get number of keywords for each class\n",
    "labels['number_of_keywords'] = labels['keywords'].apply(lambda row: len(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag_train['class']=ag_train['class'].astype(str)\n",
    "# ag_test['class']=ag_test['class'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_index</th>\n",
       "      <th>class_name</th>\n",
       "      <th>keywords</th>\n",
       "      <th>number_of_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FakeAccidents</td>\n",
       "      <td>[police,, president,, u.s.,, state,, govern, ,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RealAccidents</td>\n",
       "      <td>[crash, haiti, fire, rescue, quake, flood, pla...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_index     class_name  \\\n",
       "0            0  FakeAccidents   \n",
       "1            1  RealAccidents   \n",
       "\n",
       "                                            keywords  number_of_keywords  \n",
       "0  [police,, president,, u.s.,, state,, govern, ,...                   8  \n",
       "1  [crash, haiti, fire, rescue, quake, flood, pla...                  18  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc: document text string\n",
    "# returns tokenized document\n",
    "# strip_tags removes meta tags from the text\n",
    "# simple preprocess converts a document into a list of lowercase tokens, ignoring tokens that are too short or too long \n",
    "# simple preprocess also removes numerical values as well as punktuation characters\n",
    "def tokenize(doc):\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data set type column\n",
    "ag_train['data_set_type'] = 'train'\n",
    "ag_test['data_set_type'] = 'test'\n",
    "\n",
    "# concat train and test data\n",
    "ag_full_corpus = pd.concat([ag_train,ag_test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and tag documents combined title + description for Lbl2Vec training\n",
    "ag_full_corpus['tagged_docs'] = ag_full_corpus.apply(lambda row: TaggedDocument(tokenize(row['title'] + '. ' + row['description']), [str(row.name)]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add doc_key column\n",
    "ag_full_corpus['doc_key'] = ag_full_corpus.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add class_name column\n",
    "ag_full_corpus = ag_full_corpus.merge(labels, left_on='class', right_on='class_index', how='left').drop(['class', 'keywords'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>data_set_type</th>\n",
       "      <th>tagged_docs</th>\n",
       "      <th>doc_key</th>\n",
       "      <th>class_index</th>\n",
       "      <th>class_name</th>\n",
       "      <th>number_of_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.N. meeting in Hong Kong to draw up new conve...</td>\n",
       "      <td>HONG KONG, China (CNN) -- The United Nation's ...</td>\n",
       "      <td>train</td>\n",
       "      <td>([meeting, in, hong, kong, to, draw, up, new, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FakeAccidents</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eleven people have been killed already this mo...</td>\n",
       "      <td>(CNN) -- One 12-year-old Virginia boy was play...</td>\n",
       "      <td>train</td>\n",
       "      <td>([eleven, people, have, been, killed, already,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>FakeAccidents</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sean Mulveyhill, Kayla Narey and Austin Renaud...</td>\n",
       "      <td>Northampton, Massachusetts (CNN)  -- Three tee...</td>\n",
       "      <td>train</td>\n",
       "      <td>([sean, mulveyhill, kayla, narey, and, austin,...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>FakeAccidents</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gordon Brown: Afghanistan campaign crucial to ...</td>\n",
       "      <td>LONDON, England (CNN) -- British Prime Ministe...</td>\n",
       "      <td>train</td>\n",
       "      <td>([gordon, brown, afghanistan, campaign, crucia...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>FakeAccidents</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dave Matthews: \"I just see [racism] everywhere...</td>\n",
       "      <td>LOS ANGELES, California (CNN) -- Watching the ...</td>\n",
       "      <td>train</td>\n",
       "      <td>([dave, matthews, just, see, racism, everywher...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>FakeAccidents</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  U.N. meeting in Hong Kong to draw up new conve...   \n",
       "1  Eleven people have been killed already this mo...   \n",
       "2  Sean Mulveyhill, Kayla Narey and Austin Renaud...   \n",
       "3  Gordon Brown: Afghanistan campaign crucial to ...   \n",
       "4  Dave Matthews: \"I just see [racism] everywhere...   \n",
       "\n",
       "                                         description data_set_type  \\\n",
       "0  HONG KONG, China (CNN) -- The United Nation's ...         train   \n",
       "1  (CNN) -- One 12-year-old Virginia boy was play...         train   \n",
       "2  Northampton, Massachusetts (CNN)  -- Three tee...         train   \n",
       "3  LONDON, England (CNN) -- British Prime Ministe...         train   \n",
       "4  LOS ANGELES, California (CNN) -- Watching the ...         train   \n",
       "\n",
       "                                         tagged_docs doc_key  class_index  \\\n",
       "0  ([meeting, in, hong, kong, to, draw, up, new, ...       0            0   \n",
       "1  ([eleven, people, have, been, killed, already,...       1            0   \n",
       "2  ([sean, mulveyhill, kayla, narey, and, austin,...       2            0   \n",
       "3  ([gordon, brown, afghanistan, campaign, crucia...       3            0   \n",
       "4  ([dave, matthews, just, see, racism, everywher...       4            0   \n",
       "\n",
       "      class_name  number_of_keywords  \n",
       "0  FakeAccidents                   8  \n",
       "1  FakeAccidents                   8  \n",
       "2  FakeAccidents                   8  \n",
       "3  FakeAccidents                   8  \n",
       "4  FakeAccidents                   8  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_full_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Lbl2Vec\n",
    "\n",
    "Train a new model from scratch with the following parameters:\n",
    "* keywords_list : iterable list of lists with descriptive keywords for each topic.\n",
    "* tagged_documents : iterable list of [gensim.models.doc2vec.TaggedDocument](https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument) elements. Each element consists of one document.\n",
    "* label_names : iterable list of custom names for each label. Label names and keywords of the same topic must have the same index.\n",
    "* similarity_threshold : only documents with a higher similarity to the respective description keywords than this treshold are used to calculate the label embedding.\n",
    "* min_num_docs : minimum number of documents that are used to calculate the label embedding. \n",
    "* epochs : number of iterations over the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model with parameters\n",
    "lbl2vec_model = Lbl2Vec(keywords_list=list(labels['keywords']), tagged_documents=ag_full_corpus['tagged_docs'], label_names=list(labels['class_name']), similarity_threshold=0.30, min_num_docs=100, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 13:27:09,354 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2022-06-20 13:37:28,294 - Lbl2Vec - INFO - Train label embeddings\n",
      "2022-06-20 13:37:28,307 - Lbl2Vec - WARNING - The following keywords from the 'keywords_list' are unknown to the Doc2Vec model and therefore not used to train the model: police, president, u.s., state, govern ,iraq, family,\n",
      "2022-06-20 13:37:28,390 - Lbl2Vec - WARNING - The following keywords from the 'keywords_list' are unknown to the Doc2Vec model and therefore not used to train the model: earthquak port-au-princ\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "lbl2vec_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict document topics of documents used to train Lbl2Vec\n",
    "\n",
    "Compute similarity scores of learned document vectors from documents that were used to train the model to each of the learned label vectors. The similarity scores consist of cosine similarities and therefore have a value range of [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 13:37:28,632 - Lbl2Vec - INFO - Get document embeddings from model\n",
      "2022-06-20 13:37:28,727 - Lbl2Vec - INFO - Calculate document<->label similarities\n"
     ]
    }
   ],
   "source": [
    "# predict similarity scores\n",
    "model_docs_lbl_similarities = lbl2vec_model.predict_model_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_key</th>\n",
       "      <th>most_similar_label</th>\n",
       "      <th>highest_similarity_score</th>\n",
       "      <th>FakeAccidents</th>\n",
       "      <th>RealAccidents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>RealAccidents</td>\n",
       "      <td>0.204860</td>\n",
       "      <td>0.088236</td>\n",
       "      <td>0.204860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RealAccidents</td>\n",
       "      <td>0.367604</td>\n",
       "      <td>0.180544</td>\n",
       "      <td>0.367604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>FakeAccidents</td>\n",
       "      <td>0.240857</td>\n",
       "      <td>0.240857</td>\n",
       "      <td>0.206558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>FakeAccidents</td>\n",
       "      <td>0.341001</td>\n",
       "      <td>0.341001</td>\n",
       "      <td>0.237668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>RealAccidents</td>\n",
       "      <td>0.135777</td>\n",
       "      <td>0.085611</td>\n",
       "      <td>0.135777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc_key most_similar_label  highest_similarity_score  FakeAccidents  \\\n",
       "0       0      RealAccidents                  0.204860       0.088236   \n",
       "1       1      RealAccidents                  0.367604       0.180544   \n",
       "2       2      FakeAccidents                  0.240857       0.240857   \n",
       "3       3      FakeAccidents                  0.341001       0.341001   \n",
       "4       4      RealAccidents                  0.135777       0.085611   \n",
       "\n",
       "   RealAccidents  \n",
       "0       0.204860  \n",
       "1       0.367604  \n",
       "2       0.206558  \n",
       "3       0.237668  \n",
       "4       0.135777  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_docs_lbl_similarities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate prediction of documents used to train Lbl2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge DataFrames to compare the predicted and true topic labels\n",
    "evaluation_train = model_docs_lbl_similarities.merge(ag_full_corpus[ag_full_corpus['data_set_type']=='train'], left_on='doc_key', right_on='doc_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.357\n"
     ]
    }
   ],
   "source": [
    "y_true_train = evaluation_train['class_name']\n",
    "y_pred_train = evaluation_train['most_similar_label']\n",
    "print('F1 score:',f1_score(y_true_train, y_pred_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well then how about test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 13:37:41,336 - Lbl2Vec - INFO - Calculate document embeddings\n",
      "2022-06-20 13:38:01,701 - Lbl2Vec - INFO - Calculate document<->label similarities\n"
     ]
    }
   ],
   "source": [
    "# predict similarity scores of new test documents (they were not used during Lbl2Vec training)\n",
    "new_docs_lbl_similarities = lbl2vec_model.predict_new_docs(tagged_docs=ag_full_corpus['tagged_docs'][ag_full_corpus['data_set_type']=='test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge DataFrames to compare the predicted and true topic labels\n",
    "evaluation_test = new_docs_lbl_similarities.merge(ag_full_corpus[ag_full_corpus['data_set_type']=='test'], left_on='doc_key', right_on='doc_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.336\n"
     ]
    }
   ],
   "source": [
    "y_true_test = evaluation_test['class_name']\n",
    "y_pred_test = evaluation_test['most_similar_label']\n",
    "print('F1 score:',f1_score(y_true_test, y_pred_test, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
