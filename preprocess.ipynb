{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "Preprocessing of the dataset. First, all words are extracted and get total count, word cound and ID assigned. Then, all articles are encoded as a list of word ID's for quick analysis.\n",
    "\n",
    "The preprocessed data is used in the `Collocation.ipynb` notebook to quickly find collocation counts for words in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inspired by the following tutorial:\n",
    "\n",
    "`https://github.com/sgsinclair/alta/blob/a482d343142cba12030fea4be8f96fb77579b3ab/ipynb/utilities/Collocates.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail.py\", \"3.0.0\", split=\"train\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dataset['article']\n",
    "n_docs = len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words\n",
    "\n",
    "The first step in preprocessing the data is counting which words exist in the data and how often they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(text):\n",
    "    return re.findall(r'\\b\\w[\\w-]*\\b', text.lower()) # from the tutorial notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting how often each word occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter() # counts the total amount of words in the dataset\n",
    "big_word_list = [] # temporary cache so the counter doesn't have to be updated at each document\n",
    "\n",
    "counter_update = 1000 # the counting is faster if you don't update the counter at each document\n",
    "\n",
    "for i in tqdm(range(n_docs)):\n",
    "    tokenised = tokenise(docs[i])\n",
    "    big_word_list += tokenised\n",
    "    if i % counter_update == 0:\n",
    "        word_count += Counter(big_word_list)\n",
    "        big_word_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting how many documents contain each word at least once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count = Counter() # counts how many documents contain each token\n",
    "big_word_list = [] # temporary cache so the counter doesn't have to be updated at each document\n",
    "\n",
    "counter_update = 1000 # the counting is faster if you don't update the counter at each document\n",
    "\n",
    "for i in tqdm(range(n_docs)):\n",
    "    tokenised = tokenise(docs[i])\n",
    "    big_word_list += list(set(tokenised))\n",
    "    if i % counter_update == 0:\n",
    "        doc_count += Counter(big_word_list)\n",
    "        big_word_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(dict(word_count).keys())\n",
    "print('the dataset contains', len(all_words), 'unique words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the word statistics in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_word_count = []\n",
    "col_doc_count = []\n",
    "all_words = list(dict(word_count).keys())\n",
    "\n",
    "for i in tqdm(range(len(all_words))):\n",
    "    col_word_count.append(word_count[all_words[i]])\n",
    "    col_doc_count.append(doc_count[all_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'word_count':col_word_count, 'doc_count':col_doc_count}, index=all_words)\n",
    "df['avg_word_count'] = df['word_count'] / n_docs\n",
    "df['avg_doc_count'] = df['doc_count'] / n_docs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the DataFrame by most common words and assigning each word an ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('avg_doc_count', ascending=False)\n",
    "df['id'] = range(len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('dataset_words.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the articles\n",
    "\n",
    "In order to make finding collocations as quick as possible, the articles will be encoded as an array of word IDs. This uses the IDs assigned in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, df):\n",
    "    # encodes the article as an array of word IDs using df as a conversion table\n",
    "    text_tok = tokenise(text)\n",
    "    # words that do not occur in the encoding scheme will receive id -1\n",
    "    text_enc = np.array([df['id'][word] if word in df.index else -1 for word in text_tok])\n",
    "    return text_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the encoding for the first article in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_art_1 = encode(docs[0], df)\n",
    "enc_art_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding function to make sure the encoding worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(enc, df):\n",
    "    text_enc = [df[df['id'] == tok].index[0] for tok in enc]\n",
    "    return ' '.join(text_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the article is reconstructed (without punctuation and capitalisation, since those are removed when tokenising):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(enc_art_1, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the preprocessed articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_encode(text, df):\n",
    "    # converts the array of IDs to a string so that it can be written to CSV format\n",
    "    return ' '.join([str(i) for i in encode(text, df)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode a small portion of the dataset to see what the resulting DataFrame will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_row, end_row = 0, 1000\n",
    "\n",
    "batch_article = docs[start_row:end_row]\n",
    "batch_highlights = dataset['highlights'][start_row:end_row]\n",
    "batch_id = dataset['id'][start_row:end_row]\n",
    "batch_encoding = [str_encode(text, df) for text in batch_article]\n",
    "batch_df = pd.DataFrame({'article': batch_article, 'highlights': batch_highlights, 'endoding': batch_encoding, 'id':batch_id})\n",
    "batch_df['label'] = np.nan\n",
    "batch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell writes the entire preprocessed dataset to disk. It uses multiple files to avoid getting files that are too large.\n",
    "\n",
    "Since the Huggingface dataset contains almost 300.000 rows, this may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_size = 10000 # amount of rows per csv file\n",
    "filepath = 'test/' # map waarin het bestand wordt geschreven\n",
    "\n",
    "\n",
    "start_row = 0 # first row to start encoding\n",
    "while start_row < n_docs:\n",
    "    # calculate slice of dataset and generate file name\n",
    "    end_row = min(start_row + csv_size, n_docs) # make sure not to exceed last row\n",
    "    filename = str(start_row) + '-' + str(end_row) + '.csv'\n",
    "    print('preparing', filename)\n",
    "    \n",
    "    # process the data and store it in a DataFrame\n",
    "    batch_article = docs[start_row:end_row]\n",
    "    batch_highlights = dataset['highlights'][start_row:end_row]\n",
    "    batch_id = dataset['id'][start_row:end_row]\n",
    "    batch_encoding = [str_encode(text, df) for text in batch_article]\n",
    "    batch_df = pd.DataFrame({'article': batch_article, 'highlights': batch_highlights, 'encoding': batch_encoding, 'id':batch_id})\n",
    "    batch_df['label'] = np.nan\n",
    "    \n",
    "    # write the file to disk\n",
    "    write_to = filepath + filename\n",
    "    \n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "    batch_df.to_csv(write_to, index=True)\n",
    "    print('wrote to file:', write_to)\n",
    "    \n",
    "    start_row = end_row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
